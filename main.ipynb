{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f482c677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Lancement du Benchmark sur two_moons (Seeds: [1, 2, 3]) ===\n",
      "Génération des simulations...\n",
      "\n",
      "==================================================\n",
      " NORMALISATION : NONE\n",
      "==================================================\n",
      "\n",
      "---> Seed 1 en cours...\n",
      "  [Step     10] Loss: 1.3978 | Total Time: 2.2s\n",
      "  [Step     63] Loss: 1.0273 | Total Time: 83.2s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import equinox as eqx\n",
    "import diffrax\n",
    "import optax\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Literal\n",
    "import sbibm\n",
    "from sbibm.metrics import c2st\n",
    "from collections import defaultdict\n",
    "from flow_matching import *\n",
    "\n",
    "# Assure-toi d'importer ta propre architecture ici\n",
    "# from flow_matching import NormType, VectorFieldNetwork, train_step\n",
    "\n",
    "def _to_jax(t: torch.Tensor) -> jax.Array:\n",
    "    return jnp.array(t.numpy())\n",
    "\n",
    "def _to_torch(x: jax.Array) -> torch.Tensor:\n",
    "    return torch.from_numpy(np.array(x))\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 1. Fonction d'Inférence modifiée (pour extraire les NFE)\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "@eqx.filter_jit\n",
    "def sample_posterior_with_stats(\n",
    "    model, state, x_obs, key, num_samples, theta_dim, rtol, atol\n",
    "):\n",
    "    keys = jax.random.split(key, num_samples)\n",
    "    \n",
    "    def single_sample(k):\n",
    "        theta_0 = jax.random.normal(k, shape=(theta_dim,))\n",
    "        \n",
    "        def vector_field_wrapper(t, y, args):\n",
    "            v_t, _ = model(t, y, x_obs, state, inference=True)\n",
    "            return v_t\n",
    "        \n",
    "        sol = diffrax.diffeqsolve(\n",
    "            diffrax.ODETerm(vector_field_wrapper),\n",
    "            diffrax.Dopri5(),\n",
    "            t0=0.0, t1=1.0, dt0=0.01,\n",
    "            y0=theta_0,\n",
    "            stepsize_controller=diffrax.PIDController(rtol=rtol, atol=atol),\n",
    "            max_steps=2000\n",
    "        )\n",
    "        # NFE moyen pour Dopri5 = num_steps * 6\n",
    "        return sol.ys[-1], sol.stats[\"num_steps\"] * 6\n",
    "\n",
    "    samples, nfe_array = jax.vmap(single_sample)(keys)\n",
    "    return samples, jnp.mean(nfe_array)\n",
    "\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 2. Le Moteur de Benchmarking (Multiseed)\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "def run_benchmark(\n",
    "    task_name: str = \"two_moons\",\n",
    "    num_simulations: int = 100_000,\n",
    "    max_iterations: int = 100_000,\n",
    "    num_eval_points: int = 6,\n",
    "    eval_samples: int = 5_000,\n",
    "    batch_size: int = 256,\n",
    "    learning_rate: float = 3e-4,\n",
    "    sigma_min: float = 1e-4,\n",
    "    alpha: float = 0.0,\n",
    "    seeds: list = [42, 43, 44]  # 3 Seeds par défaut\n",
    "):\n",
    "    print(f\"=== Lancement du Benchmark sur {task_name} (Seeds: {seeds}) ===\")\n",
    "    task = sbibm.get_task(task_name)\n",
    "    theta_dim = task.dim_parameters\n",
    "    x_dim = task.dim_data\n",
    "    reference = task.get_reference_posterior_samples(num_observation=1)\n",
    "    x_obs = _to_jax(task.get_observation(num_observation=1)).squeeze(0)\n",
    "\n",
    "    print(\"Génération des simulations...\")\n",
    "    prior = task.get_prior()\n",
    "    simulator = task.get_simulator()\n",
    "    thetas = prior(num_samples=num_simulations)\n",
    "    xs = simulator(thetas)\n",
    "    theta_jax, x_jax = _to_jax(thetas), _to_jax(xs)\n",
    "\n",
    "    eval_steps = np.unique(np.logspace(1, np.log10(max_iterations), num=num_eval_points, dtype=int))\n",
    "    \n",
    "    norm_types = [\"none\", \"layer\", \"spectral\"]\n",
    "    tolerances = [1e-3, 1e-5, 1e-7]\n",
    "    \n",
    "    # Dictionnaire imbriqué : results[norm][metric][seed] = [valeurs...]\n",
    "    results = defaultdict(lambda: defaultdict(lambda: defaultdict(list)))\n",
    "    \n",
    "    for norm in norm_types:\n",
    "        print(f\"\\n\" + \"=\"*50)\n",
    "        print(f\" NORMALISATION : {norm.upper()}\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        for seed in seeds:\n",
    "            print(f\"\\n---> Seed {seed} en cours...\")\n",
    "            key = jax.random.PRNGKey(seed)\n",
    "            key_model, key_train, key_sample = jax.random.split(key, 3)\n",
    "\n",
    "            model, state = eqx.nn.make_with_state(VectorFieldNetwork)(\n",
    "                theta_dim, x_dim, [32, 128, 512, 128, 32], depth_per_block=2, key=key_model, norm_type=norm\n",
    "            )\n",
    "            optim = optax.adam(learning_rate)\n",
    "            opt_state = optim.init(eqx.filter(model, eqx.is_inexact_array))\n",
    "\n",
    "            step_losses = []\n",
    "            start_train_time = time.time()\n",
    "\n",
    "            for step in range(1, max_iterations + 1):\n",
    "                key_train, key_batch = jax.random.split(key_train)\n",
    "                idx = jax.random.randint(key_batch, (batch_size,), 0, num_simulations)\n",
    "                keys_b = jax.random.split(key_batch, batch_size)\n",
    "                \n",
    "                model, state, opt_state, loss = train_step(\n",
    "                    model, state, opt_state,\n",
    "                    theta_jax[idx], x_jax[idx], keys_b,\n",
    "                    optim, sigma_min, alpha,\n",
    "                )\n",
    "                step_losses.append(float(loss))\n",
    "\n",
    "                # --- ÉVALUATION AU PAS LOGARITHMIQUE ---\n",
    "                if step in eval_steps:\n",
    "                    avg_loss = np.mean(step_losses[-100:]) if len(step_losses) > 100 else np.mean(step_losses)\n",
    "                    print(f\"  [Step {step:6d}] Loss: {avg_loss:.4f} | Total Time: {time.time() - start_train_time:.1f}s\")\n",
    "                    \n",
    "                    results[norm][\"loss\"][seed].append(avg_loss)\n",
    "\n",
    "                    for tol in tolerances:\n",
    "                        key_sample, subkey = jax.random.split(key_sample)\n",
    "                        \n",
    "                        inf_start = time.time()\n",
    "                        samples_jax, nfe = sample_posterior_with_stats(\n",
    "                            model, state, x_obs, subkey, eval_samples, theta_dim, rtol=tol, atol=tol\n",
    "                        )\n",
    "                        samples_jax.block_until_ready()\n",
    "                        inf_time = time.time() - inf_start\n",
    "                        \n",
    "                        samples_torch = _to_torch(samples_jax)\n",
    "                        c2st_val = float(c2st(reference, samples_torch))\n",
    "                        \n",
    "                        results[norm][f\"c2st_{tol}\"][seed].append(c2st_val)\n",
    "                        results[norm][f\"nfe_{tol}\"][seed].append(float(nfe))\n",
    "                        results[norm][f\"inf_time_{tol}\"][seed].append(inf_time)\n",
    "                        \n",
    "    return results, eval_steps\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 3. Fonction de Visualisation (Agrégation Multiseed)\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "def plot_benchmark_results(results, eval_steps):\n",
    "    norm_colors = {\"none\": \"red\", \"layer\": \"blue\", \"spectral\": \"green\"}\n",
    "    tol_colors = {1e-3: \"orange\", 1e-5: \"purple\", 1e-7: \"brown\"}\n",
    "    \n",
    "    # =========================================================================\n",
    "    # FIGURE 1 : Training Loss et C2ST (tol=1e-7 uniquement)\n",
    "    # =========================================================================\n",
    "    fig1, axs1 = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    for norm in results.keys():\n",
    "        # -- 1A. Training Loss --\n",
    "        loss_matrix = np.array(list(results[norm][\"loss\"].values())) # Shape: (n_seeds, n_eval_points)\n",
    "        loss_mean = loss_matrix.mean(axis=0)\n",
    "        loss_min = loss_matrix.min(axis=0)\n",
    "        loss_max = loss_matrix.max(axis=0)\n",
    "        \n",
    "        axs1[0].plot(eval_steps, loss_mean, color=norm_colors[norm], label=norm.upper(), marker='o')\n",
    "        axs1[0].fill_between(eval_steps, loss_min, loss_max, color=norm_colors[norm], alpha=0.2)\n",
    "        \n",
    "        # -- 1B. C2ST (Uniquement tol = 1e-7) --\n",
    "        c2st_matrix = np.array(list(results[norm][\"c2st_1e-07\"].values()))\n",
    "        c2st_mean = c2st_matrix.mean(axis=0)\n",
    "        c2st_min = c2st_matrix.min(axis=0)\n",
    "        c2st_max = c2st_matrix.max(axis=0)\n",
    "        \n",
    "        axs1[1].plot(eval_steps, c2st_mean, color=norm_colors[norm], label=norm.upper(), marker='s')\n",
    "        axs1[1].fill_between(eval_steps, c2st_min, c2st_max, color=norm_colors[norm], alpha=0.2)\n",
    "\n",
    "    axs1[0].set_xscale('log')\n",
    "    axs1[0].set_yscale('log')\n",
    "    axs1[0].set_title(\"Training Loss vs Iterations (Mean & Min/Max Fill)\")\n",
    "    axs1[0].set_xlabel(\"Iterations (Log)\")\n",
    "    axs1[0].set_ylabel(\"MSE Loss (Log)\")\n",
    "    axs1[0].legend()\n",
    "    axs1[0].grid(True, which=\"both\", ls=\"--\", alpha=0.5)\n",
    "\n",
    "    axs1[1].set_xscale('log')\n",
    "    axs1[1].set_title(\"C2ST Score vs Iterations | tol=1e-7 (0.5 = Perfect)\")\n",
    "    axs1[1].set_xlabel(\"Iterations (Log)\")\n",
    "    axs1[1].set_ylabel(\"C2ST Score\")\n",
    "    axs1[1].legend()\n",
    "    axs1[1].grid(True, which=\"both\", ls=\"--\", alpha=0.5)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # =========================================================================\n",
    "    # FIGURE 2 : Grille 3x2 (NFE et Temps d'inférence)\n",
    "    # =========================================================================\n",
    "    fig2, axs2 = plt.subplots(3, 2, figsize=(16, 16))\n",
    "    norm_types = [\"none\", \"layer\", \"spectral\"]\n",
    "    tolerances = [1e-3, 1e-5, 1e-7]\n",
    "\n",
    "    for i, norm in enumerate(norm_types):\n",
    "        for tol in tolerances:\n",
    "            # -- Colonne 1 : NFE --\n",
    "            nfe_matrix = np.array(list(results[norm][f\"nfe_{tol}\"].values()))\n",
    "            nfe_mean = nfe_matrix.mean(axis=0)\n",
    "            nfe_min = nfe_matrix.min(axis=0)\n",
    "            nfe_max = nfe_matrix.max(axis=0)\n",
    "            \n",
    "            axs2[i, 0].plot(eval_steps, nfe_mean, color=tol_colors[tol], label=f\"tol={tol:.0e}\", marker='.')\n",
    "            axs2[i, 0].fill_between(eval_steps, nfe_min, nfe_max, color=tol_colors[tol], alpha=0.2)\n",
    "\n",
    "            # -- Colonne 2 : Inference Time --\n",
    "            inf_matrix = np.array(list(results[norm][f\"inf_time_{tol}\"].values()))\n",
    "            inf_mean = inf_matrix.mean(axis=0)\n",
    "            inf_min = inf_matrix.min(axis=0)\n",
    "            inf_max = inf_matrix.max(axis=0)\n",
    "            \n",
    "            axs2[i, 1].plot(eval_steps, inf_mean, color=tol_colors[tol], label=f\"tol={tol:.0e}\", marker='.')\n",
    "            axs2[i, 1].fill_between(eval_steps, inf_min, inf_max, color=tol_colors[tol], alpha=0.2)\n",
    "\n",
    "        # Formattage NFE\n",
    "        axs2[i, 0].set_xscale('log')\n",
    "        axs2[i, 0].set_yscale('log')\n",
    "        axs2[i, 0].set_title(f\"[{norm.upper()}] ODE Stiffness (NFE)\")\n",
    "        axs2[i, 0].set_xlabel(\"Iterations (Log)\")\n",
    "        axs2[i, 0].set_ylabel(\"NFE (Log)\")\n",
    "        axs2[i, 0].legend()\n",
    "        axs2[i, 0].grid(True, which=\"both\", ls=\"--\", alpha=0.5)\n",
    "\n",
    "        # Formattage Inference Time\n",
    "        axs2[i, 1].set_xscale('log')\n",
    "        axs2[i, 1].set_title(f\"[{norm.upper()}] Inference Time (Seconds)\")\n",
    "        axs2[i, 1].set_xlabel(\"Iterations (Log)\")\n",
    "        axs2[i, 1].set_ylabel(\"Time (s)\")\n",
    "        axs2[i, 1].legend()\n",
    "        axs2[i, 1].grid(True, which=\"both\", ls=\"--\", alpha=0.5)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 4. Lancement\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "results, eval_steps = run_benchmark(\n",
    "    max_iterations=100_000,\n",
    "    num_eval_points=6,\n",
    "    eval_samples=10_000,\n",
    "    batch_size=256,\n",
    "    seeds=[1, 2, 3]\n",
    ")\n",
    "\n",
    "plot_benchmark_results(results, eval_steps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
